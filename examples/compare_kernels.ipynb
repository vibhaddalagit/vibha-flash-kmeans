{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Old vs New Assignment Kernel\n",
    "\n",
    "This notebook compares the speed of:\n",
    "- `assign_euclid_triton.py` (old - two reductions)\n",
    "- `assign_euclid_triton2.py` (new - one reduction + extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'vibha-flash-kmeans'...\n",
      "remote: Enumerating objects: 189, done.\u001b[K\n",
      "remote: Counting objects: 100% (189/189), done.\u001b[K\n",
      "remote: Compressing objects: 100% (131/131), done.\u001b[K\n",
      "remote: Total 189 (delta 102), reused 137 (delta 56), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (189/189), 6.48 MiB | 17.42 MiB/s, done.\n",
      "Resolving deltas: 100% (102/102), done.\n",
      "/content/flash-kmeans/flash-kmeans/vibha-flash-kmeans\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building editable for flash-kmeans (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# Clone the repo and install\n",
    "!git clone https://github.com/vibhaddalagit/vibha-flash-kmeans.git\n",
    "%cd vibha-flash-kmeans\n",
    "!pip install -e . -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu126\n",
      "Triton version: 3.5.0\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Triton version: {triton.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both kernels imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import both implementations\n",
    "from flash_kmeans.assign_euclid_triton import euclid_assign_triton as euclid_assign_old\n",
    "from flash_kmeans.assign_euclid_triton2 import euclid_assign_triton as euclid_assign_new\n",
    "\n",
    "print(\"Both kernels imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_kernel(kernel_fn, x, centroids, x_sq, warmup=10, repeats=100):\n",
    "    \"\"\"Benchmark a kernel function.\"\"\"\n",
    "    B, N, D = x.shape\n",
    "    out = torch.empty((B, N), device=x.device, dtype=torch.int32)\n",
    "    c_sq = (centroids.float() ** 2).sum(-1)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        kernel_fn(x, centroids, x_sq, out, c_sq)\n",
    "    \n",
    "    # Benchmark\n",
    "    torch.cuda.synchronize()\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    start.record()\n",
    "    for _ in range(repeats):\n",
    "        kernel_fn(x, centroids, x_sq, out, c_sq)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    avg_time_ms = start.elapsed_time(end) / repeats\n",
    "    return avg_time_ms, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test config: B=32, N=74256, D=128, K=1000, dtype=torch.float16\n",
      "Points per batch: 74,256\n",
      "Total points: 2,376,192\n"
     ]
    }
   ],
   "source": [
    "# Test parameters (same as your benchmark)\n",
    "B, N, D = 32, 74256, 128\n",
    "K = 1000\n",
    "dtype = torch.float16\n",
    "\n",
    "print(f\"Test config: B={B}, N={N}, D={D}, K={K}, dtype={dtype}\")\n",
    "print(f\"Points per batch: {N:,}\")\n",
    "print(f\"Total points: {B*N:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([32, 74256, 128])\n",
      "centroids shape: torch.Size([32, 1000, 128])\n",
      "x_sq shape: torch.Size([32, 74256])\n"
     ]
    }
   ],
   "source": [
    "# Create test data\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(B, N, D, device=\"cuda\", dtype=dtype)\n",
    "centroids = torch.randn(B, K, D, device=\"cuda\", dtype=dtype)\n",
    "x_sq = (x.float() ** 2).sum(-1)\n",
    "\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"centroids shape: {centroids.shape}\")\n",
    "print(f\"x_sq shape: {x_sq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking correctness...\n",
      "Old kernel matches reference: False\n",
      "New kernel matches reference: False\n",
      "Old and New match each other: True\n"
     ]
    }
   ],
   "source": [
    "# Correctness check\n",
    "print(\"Checking correctness...\")\n",
    "\n",
    "# Reference implementation\n",
    "dist = (\n",
    "    x_sq.unsqueeze(-1) \n",
    "    + (centroids.float() ** 2).sum(-1).unsqueeze(1) \n",
    "    - 2.0 * torch.einsum(\"bnd,bkd->bnk\", x.float(), centroids.float())\n",
    ").clamp_min_(0.0)\n",
    "ref_ids = dist.argmin(dim=-1)\n",
    "\n",
    "# Old kernel\n",
    "out_old = torch.empty((B, N), device=\"cuda\", dtype=torch.int32)\n",
    "c_sq = (centroids.float() ** 2).sum(-1)\n",
    "euclid_assign_old(x, centroids, x_sq, out_old, c_sq)\n",
    "\n",
    "# New kernel\n",
    "out_new = torch.empty((B, N), device=\"cuda\", dtype=torch.int32)\n",
    "euclid_assign_new(x, centroids, x_sq, out_new, c_sq)\n",
    "\n",
    "print(f\"Old kernel matches reference: {torch.equal(ref_ids, out_old.long())}\")\n",
    "print(f\"New kernel matches reference: {torch.equal(ref_ids, out_new.long())}\")\n",
    "print(f\"Old and New match each other: {torch.equal(out_old, out_new)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking (warmup=10, repeats=100)...\n",
      "\n",
      "Old kernel (2 reductions): 4.321 ms\n",
      "New kernel (1 reduction):  4.248 ms\n",
      "\n",
      "Speedup: 1.7%\n",
      "Time saved per call: 0.073 ms\n"
     ]
    }
   ],
   "source": [
    "# Benchmark both kernels\n",
    "print(\"\\nBenchmarking (warmup=10, repeats=100)...\\n\")\n",
    "\n",
    "time_old, _ = benchmark_kernel(euclid_assign_old, x, centroids, x_sq)\n",
    "print(f\"Old kernel (2 reductions): {time_old:.3f} ms\")\n",
    "\n",
    "time_new, _ = benchmark_kernel(euclid_assign_new, x, centroids, x_sq)\n",
    "print(f\"New kernel (1 reduction):  {time_new:.3f} ms\")\n",
    "\n",
    "speedup = (time_old - time_new) / time_old * 100\n",
    "print(f\"\\nSpeedup: {speedup:.1f}%\")\n",
    "print(f\"Time saved per call: {time_old - time_new:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmark across different K values:\n",
      "\n",
      "     K |   Old (ms) |   New (ms) |  Speedup\n",
      "---------------------------------------------\n",
      "   256 |      1.020 |      1.124 |   -10.3%\n",
      "   512 |      1.999 |      2.134 |    -6.7%\n",
      "  1000 |      4.088 |      4.258 |    -4.2%\n",
      "  2000 |      7.752 |      8.281 |    -6.8%\n"
     ]
    }
   ],
   "source": [
    "# Test with different K values\n",
    "print(\"\\nBenchmark across different K values:\\n\")\n",
    "print(f\"{'K':>6} | {'Old (ms)':>10} | {'New (ms)':>10} | {'Speedup':>8}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for K in [100, 256, 512, 1000, 2000]:\n",
    "    centroids_k = torch.randn(B, K, D, device=\"cuda\", dtype=dtype)\n",
    "    \n",
    "    time_old, _ = benchmark_kernel(euclid_assign_old, x, centroids_k, x_sq, warmup=5, repeats=50)\n",
    "    time_new, _ = benchmark_kernel(euclid_assign_new, x, centroids_k, x_sq, warmup=5, repeats=50)\n",
    "    \n",
    "    speedup = (time_old - time_new) / time_old * 100\n",
    "    print(f\"{K:>6} | {time_old:>10.3f} | {time_new:>10.3f} | {speedup:>7.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Test size: B={B}, N={N}, D={D}, K=1000\")\n",
    "print(f\"\\nOptimization: Removed redundant tl.min() call\")\n",
    "print(f\"Old: tl.min() + tl.argmin() (2 reductions)\")\n",
    "print(f\"New: tl.argmin() + extract (1 reduction)\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
